{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pickle\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "ON_TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Functions:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def make_cov_matrix(self, sigma=None, corr=None, chol=None, module=np):\n",
    "        if chol is None:\n",
    "            C = module.ones((2, 2))\n",
    "            var = module.diag(sigma)\n",
    "            idxu = np.triu_indices(2, 1)\n",
    "            idxl = np.tril_indices(2, -1)\n",
    "            if module == np:\n",
    "                C[idxu] = corr\n",
    "                C[idxl] = corr\n",
    "            else:\n",
    "                C = tt.set_subtensor(C[idxu], corr)\n",
    "                C = tt.set_subtensor(C[idxl], corr)\n",
    "            cov = var.dot(C).dot(var)\n",
    "        else:\n",
    "            C = module.zeros((2, 2))\n",
    "            idxl = np.mask_indices(2, np.tril)\n",
    "            if module == np:\n",
    "                C[idxl] = chol\n",
    "            else:\n",
    "                C = tt.set_subtensor(C[idxl], chol)\n",
    "            cov = C.dot(C.T)\n",
    "        return cov\n",
    "\n",
    "    def sampling(self, w, mu, cov, N=2000):\n",
    "        num = int(w * N)\n",
    "        if len(mu) == 1:\n",
    "            samples = np.random.normal(mu, cov, size=num)\n",
    "        else:\n",
    "            samples = np.random.multivariate_normal(mu, cov, size=num)\n",
    "        return samples\n",
    "\n",
    "    def flatten(self, input_list):\n",
    "        output_list = []\n",
    "        while True:\n",
    "            if input_list == []:\n",
    "                break\n",
    "            for index, i in enumerate(input_list):\n",
    "                if type(i) == list:\n",
    "                    input_list = i + input_list[index + 1:]\n",
    "                    break\n",
    "                else:\n",
    "                    output_list.append(i)\n",
    "                    input_list.pop(index)\n",
    "                    break\n",
    "        return output_list\n",
    "\n",
    "    def norm_1d(self, x, mu, sigma):\n",
    "        prob = np.zeros(x.shape[0])\n",
    "        for i, v in enumerate(x):\n",
    "            prob[i] = 1 / (np.sqrt(2 * 3.14 * sigma ** 2)) * np.exp(-0.5 * (v - mu) ** 2 / sigma ** 2)\n",
    "        return prob\n",
    "\n",
    "    def norm_2d(self, x, w, mu, cov):\n",
    "        prob = np.zeros(x.shape[0])\n",
    "        for i, v in enumerate(x):\n",
    "            c = [np.sqrt(1 / ((2 * 3.14) ** 2 * np.linalg.det(cov[k]))) for k in range(K)]\n",
    "            f = [c[k] * np.exp(-0.5 * (v - mu[k]).dot(np.linalg.inv(cov[k])).dot((v - mu[k]).T)) for k in range(K)]\n",
    "            prob[i] = np.sum([w[k] * f[k] for k in range(K)])\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LoadData:\n",
    "    def __init__(self):\n",
    "        self.data = self.read_file()\n",
    "\n",
    "    def read_file(self):\n",
    "        header_1 = ['time', 'headway', 'ttc', 'num', 'speed']\n",
    "        header_2 = ['time', 'maneuver', 'lat', 'lon', 'duration', 'th']\n",
    "        time = []\n",
    "        hs = []\n",
    "        manu = []\n",
    "        for root, dirs, file in os.walk('./UAH/D2/'):\n",
    "            for path in dirs:\n",
    "                detection = pd.read_csv(os.path.join(root, path, 'PROC_VEHICLE_DETECTION.txt'), sep='\\s+', names=header_1)\n",
    "                event = pd.read_csv(os.path.join(root, path, 'EVENTS_LIST_LANE_CHANGES.txt'), sep='\\s+', names=header_2)\n",
    "                time_1 = np.around(detection['time'].values, 1)\n",
    "                time_2 = np.around(event['time'].values, 1)\n",
    "                headway = detection['headway'].values\n",
    "                t = np.arange(time_1[0], time_1[-1], 0.1)\n",
    "                s = np.interp(t, time_1, headway)\n",
    "                index = []\n",
    "                for d in time_2:\n",
    "                    index.append(np.where(np.abs(t - d) <= 1e-3)[0][0])\n",
    "                m = np.zeros_like(t)\n",
    "                m[index] = np.sign(event['maneuver'].values)\n",
    "                time.append(t)\n",
    "                hs.append(s)\n",
    "                manu.append(m)\n",
    "        time = np.concatenate(time)\n",
    "        hs = np.concatenate(hs)\n",
    "        manu = np.concatenate(manu)\n",
    "        x0, dx0, m0 = self.grad(time, hs, manu)\n",
    "        return x0, dx0, m0\n",
    "\n",
    "    def grad(self, t, x, m):\n",
    "        def outlier(s, x, level=1):\n",
    "            mean_x = np.mean(s)\n",
    "            std_x = np.std(s)\n",
    "            index = np.where(np.abs(s - mean_x) >= level * std_x)\n",
    "            x = [np.delete(x[k], index) for k in range(len(x))]\n",
    "            return x\n",
    "\n",
    "        def kalman(obs, x, p=np.eye(2), dt=0.1):\n",
    "            eta = []\n",
    "            for i in range(len(obs)):\n",
    "                A = np.array([[1, dt], [0, 1]])\n",
    "                C = np.array([[1, 0]])\n",
    "                Q = 100 * np.eye(2)\n",
    "                R = 1\n",
    "                x = A.dot(x)\n",
    "                p = A.dot(p).dot(A.T) + Q\n",
    "                k = p.dot(C.T).dot(np.linalg.inv(C.dot(p).dot(C.T) + R))\n",
    "                x = x + k.dot(obs[i] - C.dot(x))\n",
    "                p = (np.eye(2) - k.dot(C)).dot(p)\n",
    "                eta.append(x[1])\n",
    "            return np.array(eta)\n",
    "\n",
    "        headway = []\n",
    "        speed = []\n",
    "        maneuver = []\n",
    "        i_0 = []\n",
    "        i_f = []\n",
    "        for i in range(len(x) - 1):\n",
    "            if x[i] == -1 and x[i + 1] != -1:\n",
    "                i_0.append(i)\n",
    "            if x[i] != -1 and x[i + 1] == -1:\n",
    "                i_f.append(i)\n",
    "        num = np.min([len(i_0), len(i_f)])\n",
    "        for i in range(num):\n",
    "            if len(x[i_0[i] + 1: i_f[i] + 1]) < 2:\n",
    "                continue\n",
    "            s = x[i_0[i] + 1: i_f[i] + 1]\n",
    "            ds1 = np.gradient(x[i_0[i] + 1: i_f[i] + 1], t[i_0[i] + 1: i_f[i] + 1])\n",
    "            ds2 = kalman(s, np.array([s[0], ds1[0]]))\n",
    "            headway.append(s)\n",
    "            speed.append(ds2)\n",
    "            maneuver.append(m[i_0[i] + 1: i_f[i] + 1])\n",
    "        headway = np.concatenate(headway)\n",
    "        speed = np.concatenate(speed)\n",
    "        maneuver = np.concatenate(maneuver)\n",
    "        headway, speed, maneuver = outlier(headway, (headway, speed, maneuver), 1)\n",
    "        headway, speed, maneuver = outlier(speed, (headway, speed, maneuver), 1)\n",
    "        return headway, speed, maneuver\n",
    "\n",
    "    def prior_samples(self, percent=0.1):\n",
    "        data_size = len(self.data[0])\n",
    "        num = int(data_size * percent)\n",
    "        index = np.random.choice(data_size, num, False)\n",
    "        # index = range(num)\n",
    "        x = self.data[0][index]\n",
    "        dx = self.data[1][index]\n",
    "        m = self.data[2][index]\n",
    "        data_prior = pd.DataFrame([x, dx]).transpose()\n",
    "        data_prior = data_prior.dropna(axis=0)\n",
    "        data_prior.columns = ['x', 'dx']\n",
    "        #         plt.figure()\n",
    "        #         plt.plot(data_prior['x'])\n",
    "        #         plt.figure()\n",
    "        #         plt.plot(data_prior['dx'])\n",
    "        #         plt.figure()\n",
    "        #         sns.heatmap(data_prior.corr(), annot=True)\n",
    "        return data_prior, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BayesianGMM:\n",
    "    def __init__(self, data):\n",
    "        self.model = pm.Model()\n",
    "        self.modeling(data)\n",
    "\n",
    "    def modeling(self, data):\n",
    "        with self.model:\n",
    "            if K == 1:\n",
    "                mu = pm.Normal('mu0', mu=[20, 0], sd=5, shape=2)\n",
    "                tril = pm.LKJCholeskyCov('chol0', n=2, eta=1, sd_dist=pm.HalfCauchy.dist(2.5))\n",
    "                chol = pm.expand_packed_triangular(2, tril)\n",
    "                obs = pm.MvNormal('obs', mu=mu, chol=chol, observed=data)\n",
    "            else:\n",
    "                w = pm.Dirichlet('w', a=np.ones(K))\n",
    "                mu = tt.stack([pm.Normal('mu' + str(k), mu=[20, 0], sd=10, shape=2) for k in range(K)])\n",
    "                tril = tt.stack([pm.LKJCholeskyCov('chol' + str(k), n=2, eta=1,\n",
    "                                                   sd_dist=pm.HalfCauchy.dist(2.5)) for k in range(K)])\n",
    "                chol = tt.stack([pm.expand_packed_triangular(2, tril[k]) for k in range(K)])\n",
    "                dist = [pm.MvNormal.dist(mu=mu[k], chol=chol[k]) for k in range(K)]\n",
    "                obs = pm.Mixture('obs', w=w, comp_dists=dist, observed=data)\n",
    "\n",
    "    def training(self, advi=False):\n",
    "        with self.model:\n",
    "            if advi:\n",
    "                inference = pm.fit(method='advi')\n",
    "                trace = inference.sample()\n",
    "            else:\n",
    "                trace = pm.sample(chains=1)\n",
    "        pm.summary(trace)\n",
    "        return trace\n",
    "\n",
    "    def plot(self, trace, type=0):\n",
    "        with self.model:\n",
    "            if type == 0:\n",
    "                pm.traceplot(trace)\n",
    "            elif type == 1:\n",
    "                pm.plot_posterior(trace)\n",
    "            elif type == 2:\n",
    "                pm.forestplot(trace)\n",
    "            elif type == 3:\n",
    "                pm.autocorrplot(trace)\n",
    "            elif type == 4:\n",
    "                pm.energyplot(trace)\n",
    "            elif type == 5:\n",
    "                pm.densityplot(trace)\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PostProcess(Functions):\n",
    "    def __init__(self, data, trace):\n",
    "        super(PostProcess, self).__init__()\n",
    "        self.data = data\n",
    "        self.trace = trace\n",
    "        self.var, self.var_mu, self.var_std, self.var_name = self.stats()\n",
    "        self.gmm = GaussianMixture(K).fit(data)\n",
    "\n",
    "    def stats(self):\n",
    "        order = np.argsort(self.trace['w'].mean(0))\n",
    "        var_name = [['mu' + str(k) for k in order], ['chol' + str(k) for k in order]]\n",
    "        var_name = self.flatten(var_name)\n",
    "        var = [np.vstack([self.trace['w'][:, k] for k in order]).T]\n",
    "        var_mu = [var[0].mean(0)]\n",
    "        var_std = [var[0].std(0)]\n",
    "\n",
    "        for name in var_name:\n",
    "            var.append(self.trace[name])\n",
    "            var_mu.append(self.trace[name].mean(0))\n",
    "            var_std.append(self.trace[name].std(0))\n",
    "        var_name.insert(0, 'w')\n",
    "        return var, var_mu, var_std, var_name\n",
    "\n",
    "    def post_samples(self):\n",
    "        mu = [self.var_mu[k] for k in range(1, 1 + K)]\n",
    "        cov = [self.make_cov_matrix(chol=self.var_mu[k]) for k in range(1 + K, 1 + 2 * K)]\n",
    "        samples = [self.sampling(self.var_mu[0][k], mu[k], cov[k]) for k in range(K)]\n",
    "        fig = plt.figure()\n",
    "        [plt.scatter(samples[k][:, 0], samples[k][:, 1], label='component' + str(k)) for k in range(K)]\n",
    "        plt.legend()\n",
    "        plt.xlim([0, 60])\n",
    "        plt.ylim([-30, 30])\n",
    "        fig.savefig('./img/post_samples', format='svg')\n",
    "        data_post = np.concatenate(samples)\n",
    "        data_post = pd.DataFrame(data_post)\n",
    "        data_post.columns = ['x', 'dx']\n",
    "        return data_post\n",
    "\n",
    "    def joint_dist(self, data_post):\n",
    "        fig = plt.figure()\n",
    "        sns.jointplot('x', 'dx', data=self.data, xlim=[5, 45], ylim=[-20, 20], kind='kde', space=0, color='r')\n",
    "        sns.jointplot('x', 'dx', data=data_post, xlim=[5, 45], ylim=[-20, 20], kind='kde', space=0, color='g')\n",
    "        fig.savefig('./img/joint_dist', format='svg')\n",
    "\n",
    "    def marginalized_dist(self, data_post):\n",
    "        fig = plt.figure()\n",
    "        plt.subplot(121)\n",
    "        sns.distplot(self.data['x'], bins=30, kde=False, norm_hist=True,\n",
    "                     hist_kws={'histtype': 'step', 'linewidth': 3}, label='Prior distribution')\n",
    "        sns.distplot(data_post['x'], bins=30, kde=False, norm_hist=True,\n",
    "                     hist_kws={'histtype': 'step', 'linewidth': 3}, label='Posterior distribution')\n",
    "        plt.legend()\n",
    "        plt.xlim([0, 60])\n",
    "        plt.subplot(122)\n",
    "        sns.distplot(self.data['dx'], bins=30, kde=False, norm_hist=True,\n",
    "                     hist_kws={'histtype': 'step', 'linewidth': 3}, label='Prior distribution')\n",
    "        sns.distplot(data_post['dx'], bins=30, kde=False, norm_hist=True,\n",
    "                     hist_kws={'histtype': 'step', 'linewidth': 3}, label='Posterior distribution')\n",
    "        plt.legend()\n",
    "        plt.xlim([-30, 30])\n",
    "        fig.savefig('./img/marginalized_dist', format='svg')\n",
    "\n",
    "    def ordered(self):\n",
    "        order = list(permutations(range(K)))\n",
    "        mse = np.zeros(len(order))\n",
    "        ixl = np.tril_indices(2)\n",
    "        for n, l in enumerate(order):\n",
    "            gmm_map = [self.gmm.weights_]\n",
    "            for k in l:\n",
    "                gmm_map.append(self.gmm.means_[k])\n",
    "            for k in l:\n",
    "                L = np.linalg.cholesky(self.gmm.covariances_[k])\n",
    "                chol = L[ixl]\n",
    "                gmm_map.append(chol)\n",
    "            value = 0.\n",
    "            for i in range(len(gmm_map)):\n",
    "                for (x, y) in zip(self.var_mu, gmm_map):\n",
    "                    value += np.sum((x - y) ** 2)\n",
    "            mse[n] = value\n",
    "        index = mse.argmin()\n",
    "        gmm_map = []\n",
    "        gmm_map.append(np.array([self.gmm.weights_[k] for k in order[index]]))\n",
    "        for k in order[index]:\n",
    "            gmm_map.append(self.gmm.means_[k])\n",
    "        for k in order[index]:\n",
    "            L = np.linalg.cholesky(self.gmm.covariances_[k])\n",
    "            chol = L[ixl]\n",
    "            gmm_map.append(chol)\n",
    "        return gmm_map\n",
    "\n",
    "    def compare_1d(self):\n",
    "        gmm_map = self.ordered()\n",
    "        fig = plt.figure()\n",
    "        for i in range(len(gmm_map)):\n",
    "            plt.subplot2grid([len(gmm_map), 2], [i, 0])\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title(self.var_name[i])\n",
    "            col = self.var[i].shape[1]\n",
    "            x = [np.linspace(self.var[i][:, k].min(), self.var[i][:, k].max(), 50) for k in range(col)]\n",
    "            p = [self.norm_1d(x[k], self.var_mu[i][k], self.var_std[i][k]) for k in range(col)]\n",
    "            p_max = [p[k].max() for k in range(col)]\n",
    "            [plt.plot(x[k], p[k]) for k in range(col)]\n",
    "            [plt.plot([gmm_map[i][k], gmm_map[i][k]], [0, p_max[k]], color='r') for k in range(col)]\n",
    "            if i == 0:\n",
    "                plt.xlim([0, 1])\n",
    "            elif i > 0 and i <= K:\n",
    "                plt.xlim([-1, 30])\n",
    "            else:\n",
    "                plt.xlim([-1, 15])\n",
    "            plt.subplot2grid([len(gmm_map), 2], [i, 1])\n",
    "            plt.ylabel('Sample value')\n",
    "            plt.title(self.var_name[i])\n",
    "            plt.plot(self.var[i])\n",
    "        fig.savefig('./img/compare_1d', format='svg')\n",
    "\n",
    "    def compare_2d(self):\n",
    "        mu = [self.var_mu[k] for k in range(1, 1 + K)]\n",
    "        cov = [self.make_cov_matrix(chol=self.var_mu[k]) for k in range(1 + K, 1 + 2 * K)]\n",
    "        p1 = self.norm_2d(self.data.values, self.gmm.weights_, self.gmm.means_, self.gmm.covariances_)\n",
    "        p2 = self.norm_2d(self.data.values, self.var_mu[0], mu, cov)\n",
    "        p1 = p1 / np.sum(p1)\n",
    "        p2 = p2 / np.sum(p2)\n",
    "        fig = plt.figure()\n",
    "        plt.subplot(121)\n",
    "        plt.scatter(self.data['x'], self.data['dx'], c=p1)\n",
    "        plt.colorbar()\n",
    "        plt.xlim([0, 60])\n",
    "        plt.ylim([-30, 30])\n",
    "        plt.subplot(122)\n",
    "        plt.scatter(self.data['x'], self.data['dx'], c=p2)\n",
    "        plt.colorbar()\n",
    "        plt.xlim([0, 60])\n",
    "        plt.ylim([-30, 30])\n",
    "        fig.savefig('./img/compare_2d', format='svg')\n",
    "        return p1\n",
    "\n",
    "    def step(self):\n",
    "        fig = plt.figure(1)\n",
    "        for i in range(len(self.var)):\n",
    "            plt.subplot(711+i)\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title(self.var_name[i])\n",
    "            col = self.var[i].shape[1]\n",
    "            x = [np.linspace(self.var[i][:, k].min(), self.var[i][:, k].max(), 50) for k in range(col)]\n",
    "            p = [self.norm_1d(x[k], self.var_mu[i][k], self.var_std[i][k]) for k in range(col)]\n",
    "            [plt.plot(x[k], p[k]) for k in range(col)]\n",
    "        # fig.savefig('./img/step', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Estimation:\n",
    "    def __init__(self, data, m):\n",
    "        self.data = data\n",
    "        self.m = m\n",
    "\n",
    "    def importance_sampling(self, prob):\n",
    "        N = 5000\n",
    "        index = np.random.choice(self.data.shape[0], N, True, prob)\n",
    "        samples = self.data.iloc[index]\n",
    "        s = self.m[index]\n",
    "        fig = plt.figure()\n",
    "        sns.scatterplot(x='x', y='dx', data=samples, alpha=0.1)\n",
    "        plt.scatter(samples['x'].iloc[s != 0], samples['dx'].iloc[s != 0], label='lane change')\n",
    "        plt.xlim([5, 45])\n",
    "        plt.ylim([-10, 10])\n",
    "        plt.legend()\n",
    "        fig.savefig('./img/importance_sampling', format='svg')\n",
    "        print(samples['x'].iloc[s != 0].shape[0] / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangwei/anaconda3/envs/theano_env/lib/python3.6/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/home/yangwei/anaconda3/envs/theano_env/lib/python3.6/site-packages/theano/tensor/subtensor.py:2339: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[0][inputs[2:]] = inputs[1]\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "/home/yangwei/anaconda3/envs/theano_env/lib/python3.6/site-packages/theano/tensor/basic.py:6611: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result[diagonal_slice] = x\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [chol2, chol1, chol0, mu2, mu1, mu0, w]\n",
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\r",
      "  0%|          | 1/1000 [00:00<01:47,  9.30it/s]\r",
      "  2%|▏         | 19/1000 [00:00<01:15, 12.93it/s]\r",
      "  3%|▎         | 32/1000 [00:00<00:55, 17.31it/s]\r",
      "  4%|▍         | 38/1000 [00:00<01:01, 15.54it/s]\r",
      "  4%|▍         | 43/1000 [00:01<01:45,  9.10it/s]\r",
      "  5%|▍         | 47/1000 [00:03<02:42,  5.87it/s]\r",
      "  5%|▌         | 50/1000 [00:04<03:19,  4.76it/s]\r",
      "  5%|▌         | 52/1000 [00:04<03:12,  4.91it/s]\r",
      "  5%|▌         | 54/1000 [00:04<03:08,  5.01it/s]\r",
      "  6%|▌         | 56/1000 [00:05<03:06,  5.07it/s]\r",
      "  6%|▌         | 57/1000 [00:05<02:43,  5.78it/s]\r",
      "  6%|▌         | 61/1000 [00:05<02:08,  7.32it/s]\r",
      "  6%|▋         | 63/1000 [00:05<01:44,  8.94it/s]\r",
      "  6%|▋         | 65/1000 [00:05<01:57,  7.97it/s]\r",
      "  7%|▋         | 67/1000 [00:06<02:24,  6.44it/s]\r",
      "  7%|▋         | 69/1000 [00:06<02:10,  7.16it/s]\r",
      "  7%|▋         | 72/1000 [00:06<01:44,  8.86it/s]\r",
      "  7%|▋         | 74/1000 [00:07<01:50,  8.39it/s]\r",
      "  8%|▊         | 76/1000 [00:07<01:48,  8.48it/s]\r",
      "  8%|▊         | 78/1000 [00:07<02:04,  7.38it/s]\r",
      "  8%|▊         | 81/1000 [00:07<01:37,  9.44it/s]\r",
      "  8%|▊         | 83/1000 [00:07<01:27, 10.46it/s]\r",
      "  9%|▊         | 86/1000 [00:08<01:10, 12.92it/s]\r",
      "  9%|▉         | 88/1000 [00:08<01:32,  9.91it/s]\r",
      "  9%|▉         | 90/1000 [00:08<01:18, 11.57it/s]\r",
      "  9%|▉         | 92/1000 [00:08<01:21, 11.15it/s]\r",
      "  9%|▉         | 94/1000 [00:08<01:19, 11.40it/s]\r",
      " 10%|▉         | 96/1000 [00:08<01:15, 11.94it/s]\r",
      " 10%|▉         | 98/1000 [00:09<01:16, 11.79it/s]\r",
      " 10%|█         | 102/1000 [00:09<01:03, 14.16it/s]\r",
      " 10%|█         | 105/1000 [00:09<00:56, 15.76it/s]\r",
      " 11%|█         | 107/1000 [00:09<01:06, 13.47it/s]\r",
      " 11%|█         | 109/1000 [00:09<01:01, 14.38it/s]\r",
      " 11%|█         | 111/1000 [00:09<01:08, 12.89it/s]\r",
      " 11%|█▏        | 113/1000 [00:10<01:03, 13.88it/s]\r",
      " 12%|█▏        | 115/1000 [00:10<01:00, 14.71it/s]\r",
      " 12%|█▏        | 117/1000 [00:10<01:04, 13.77it/s]\r",
      " 12%|█▏        | 119/1000 [00:10<01:28,  9.99it/s]\r",
      " 12%|█▏        | 121/1000 [00:10<01:32,  9.48it/s]\r",
      " 12%|█▏        | 123/1000 [00:11<01:26, 10.11it/s]\r",
      " 13%|█▎        | 126/1000 [00:11<01:10, 12.32it/s]\r",
      " 13%|█▎        | 128/1000 [00:11<01:09, 12.59it/s]\r",
      " 13%|█▎        | 130/1000 [00:11<01:37,  8.89it/s]\r",
      " 13%|█▎        | 132/1000 [00:11<01:26, 10.01it/s]\r",
      " 14%|█▎        | 135/1000 [00:11<01:12, 11.86it/s]\r",
      " 14%|█▎        | 137/1000 [00:12<01:14, 11.55it/s]\r",
      " 14%|█▍        | 139/1000 [00:12<01:10, 12.13it/s]\r",
      " 14%|█▍        | 141/1000 [00:12<01:08, 12.53it/s]\r",
      " 14%|█▍        | 144/1000 [00:12<01:10, 12.17it/s]\r",
      " 15%|█▍        | 146/1000 [00:12<01:04, 13.20it/s]\r",
      " 15%|█▍        | 148/1000 [00:12<00:58, 14.49it/s]\r",
      " 15%|█▌        | 150/1000 [00:13<00:57, 14.72it/s]\r",
      " 15%|█▌        | 152/1000 [00:13<01:01, 13.88it/s]\r",
      " 16%|█▌        | 155/1000 [00:13<00:58, 14.50it/s]\r",
      " 16%|█▌        | 157/1000 [00:13<00:59, 14.22it/s]\r",
      " 16%|█▌        | 160/1000 [00:13<00:54, 15.55it/s]\r",
      " 16%|█▌        | 162/1000 [00:13<01:05, 12.84it/s]\r",
      " 16%|█▋        | 164/1000 [00:14<01:00, 13.71it/s]\r",
      " 17%|█▋        | 166/1000 [00:14<00:55, 14.99it/s]\r",
      " 17%|█▋        | 169/1000 [00:14<01:03, 13.00it/s]\r",
      " 17%|█▋        | 172/1000 [00:14<01:00, 13.67it/s]\r",
      " 18%|█▊        | 175/1000 [00:14<00:52, 15.64it/s]\r",
      " 18%|█▊        | 177/1000 [00:14<01:00, 13.68it/s]\r",
      " 18%|█▊        | 179/1000 [00:15<00:56, 14.42it/s]\r",
      " 18%|█▊        | 183/1000 [00:15<00:47, 17.09it/s]\r",
      " 19%|█▊        | 186/1000 [00:15<00:47, 16.98it/s]\r",
      " 19%|█▉        | 189/1000 [00:15<00:48, 16.56it/s]\r",
      " 19%|█▉        | 192/1000 [00:15<00:53, 15.12it/s]\r",
      " 19%|█▉        | 194/1000 [00:16<01:00, 13.38it/s]\r",
      " 20%|█▉        | 196/1000 [00:16<00:56, 14.31it/s]\r",
      " 20%|█▉        | 199/1000 [00:16<00:48, 16.47it/s]\r",
      " 20%|██        | 201/1000 [00:16<00:46, 17.22it/s]\r",
      " 20%|██        | 204/1000 [00:16<00:47, 16.88it/s]\r",
      " 21%|██        | 206/1000 [00:16<01:23,  9.55it/s]\r",
      " 21%|██        | 208/1000 [00:17<01:20,  9.78it/s]\r",
      " 21%|██        | 210/1000 [00:17<01:17, 10.17it/s]\r",
      " 21%|██▏       | 214/1000 [00:17<01:01, 12.84it/s]\r",
      " 22%|██▏       | 221/1000 [00:17<00:46, 16.87it/s]\r",
      " 23%|██▎       | 228/1000 [00:17<00:36, 21.39it/s]\r",
      " 24%|██▎       | 236/1000 [00:17<00:28, 27.28it/s]\r",
      " 24%|██▍       | 244/1000 [00:17<00:22, 33.55it/s]\r",
      " 25%|██▌       | 251/1000 [00:18<00:18, 39.69it/s]\r",
      " 26%|██▌       | 258/1000 [00:18<00:16, 44.89it/s]\r",
      " 27%|██▋       | 267/1000 [00:18<00:13, 52.73it/s]\r",
      " 28%|██▊       | 275/1000 [00:18<00:12, 58.16it/s]\r",
      " 28%|██▊       | 283/1000 [00:18<00:11, 59.81it/s]\r",
      " 32%|███▏      | 320/1000 [00:18<00:08, 79.42it/s]\r",
      " 34%|███▎      | 336/1000 [00:18<00:08, 80.54it/s]\r",
      " 35%|███▌      | 350/1000 [00:18<00:08, 75.33it/s]\r",
      " 36%|███▌      | 362/1000 [00:19<00:08, 74.75it/s]\r",
      " 37%|███▋      | 372/1000 [00:19<00:09, 65.54it/s]\r",
      " 38%|███▊      | 381/1000 [00:19<00:09, 64.09it/s]\r",
      " 39%|███▉      | 389/1000 [00:19<00:09, 66.46it/s]\r",
      " 40%|███▉      | 397/1000 [00:19<00:08, 68.27it/s]\r",
      " 40%|████      | 405/1000 [00:19<00:08, 68.12it/s]\r",
      " 41%|████▏     | 414/1000 [00:19<00:07, 73.43it/s]\r",
      " 42%|████▏     | 423/1000 [00:20<00:07, 77.22it/s]\r",
      " 43%|████▎     | 432/1000 [00:20<00:07, 77.57it/s]\r",
      " 44%|████▍     | 441/1000 [00:20<00:07, 79.66it/s]\r",
      " 45%|████▌     | 450/1000 [00:20<00:08, 64.82it/s]\r",
      " 46%|████▌     | 459/1000 [00:20<00:07, 69.12it/s]\r",
      " 47%|████▋     | 467/1000 [00:20<00:07, 71.09it/s]\r",
      " 48%|████▊     | 475/1000 [00:20<00:08, 64.51it/s]\r",
      " 48%|████▊     | 482/1000 [00:20<00:08, 60.38it/s]\r",
      " 49%|████▉     | 490/1000 [00:21<00:08, 63.32it/s]\r",
      " 50%|████▉     | 497/1000 [00:21<00:08, 57.74it/s]\r",
      " 50%|█████     | 504/1000 [00:21<00:09, 53.96it/s]\r",
      " 51%|█████     | 512/1000 [00:21<00:08, 58.67it/s]\r",
      " 52%|█████▏    | 519/1000 [00:21<00:07, 60.85it/s]\r",
      " 53%|█████▎    | 528/1000 [00:21<00:07, 66.29it/s]\r",
      " 54%|█████▎    | 536/1000 [00:21<00:06, 69.00it/s]\r",
      " 54%|█████▍    | 544/1000 [00:21<00:06, 69.69it/s]\r",
      " 55%|█████▌    | 552/1000 [00:21<00:06, 72.26it/s]\r",
      " 56%|█████▌    | 560/1000 [00:22<00:06, 71.94it/s]\r",
      " 57%|█████▋    | 568/1000 [00:22<00:06, 69.13it/s]\r",
      " 58%|█████▊    | 576/1000 [00:22<00:06, 68.69it/s]\r",
      " 58%|█████▊    | 583/1000 [00:22<00:06, 67.75it/s]\r",
      " 59%|█████▉    | 590/1000 [00:22<00:06, 60.05it/s]\r",
      " 60%|█████▉    | 597/1000 [00:22<00:06, 60.87it/s]\r",
      " 60%|██████    | 604/1000 [00:22<00:06, 60.61it/s]\r",
      " 61%|██████    | 611/1000 [00:22<00:06, 62.82it/s]\r",
      " 62%|██████▏   | 620/1000 [00:23<00:05, 67.33it/s]\r",
      " 63%|██████▎   | 628/1000 [00:23<00:05, 69.78it/s]\r",
      " 64%|██████▎   | 636/1000 [00:23<00:05, 65.11it/s]\r",
      " 64%|██████▍   | 643/1000 [00:23<00:05, 66.40it/s]\r",
      " 65%|██████▌   | 650/1000 [00:23<00:05, 64.22it/s]\r",
      " 66%|██████▌   | 657/1000 [00:23<00:05, 63.76it/s]\r",
      " 66%|██████▋   | 665/1000 [00:23<00:04, 67.10it/s]\r",
      " 67%|██████▋   | 672/1000 [00:23<00:05, 63.66it/s]\r",
      " 68%|██████▊   | 680/1000 [00:23<00:04, 65.59it/s]\r",
      " 69%|██████▉   | 688/1000 [00:24<00:04, 68.62it/s]\r",
      " 70%|██████▉   | 697/1000 [00:24<00:04, 73.11it/s]\r",
      " 70%|███████   | 705/1000 [00:24<00:04, 69.44it/s]\r",
      " 71%|███████▏  | 713/1000 [00:24<00:04, 71.45it/s]\r",
      " 72%|███████▏  | 722/1000 [00:24<00:03, 74.31it/s]\r",
      " 73%|███████▎  | 730/1000 [00:24<00:03, 73.52it/s]\r",
      " 74%|███████▍  | 738/1000 [00:24<00:03, 72.29it/s]\r",
      " 75%|███████▍  | 747/1000 [00:24<00:03, 75.00it/s]\r",
      " 76%|███████▌  | 755/1000 [00:24<00:03, 71.51it/s]\r",
      " 76%|███████▋  | 763/1000 [00:25<00:03, 71.23it/s]\r",
      " 77%|███████▋  | 771/1000 [00:25<00:03, 73.21it/s]\r",
      " 78%|███████▊  | 779/1000 [00:25<00:03, 68.14it/s]\r",
      " 79%|███████▊  | 787/1000 [00:25<00:03, 69.16it/s]\r",
      " 80%|███████▉  | 795/1000 [00:25<00:02, 71.21it/s]\r",
      " 80%|████████  | 803/1000 [00:25<00:02, 72.18it/s]\r",
      " 81%|████████  | 811/1000 [00:25<00:02, 69.74it/s]\r",
      " 82%|████████▏ | 819/1000 [00:25<00:02, 72.14it/s]\r",
      " 83%|████████▎ | 827/1000 [00:25<00:02, 70.87it/s]\r",
      " 84%|████████▎ | 835/1000 [00:26<00:02, 69.74it/s]\r",
      " 84%|████████▍ | 844/1000 [00:26<00:02, 72.88it/s]\r",
      " 85%|████████▌ | 852/1000 [00:26<00:02, 71.79it/s]\r",
      " 86%|████████▌ | 860/1000 [00:26<00:01, 73.65it/s]\r",
      " 87%|████████▋ | 868/1000 [00:26<00:01, 70.23it/s]\r",
      " 88%|████████▊ | 876/1000 [00:26<00:01, 68.04it/s]\r",
      " 88%|████████▊ | 883/1000 [00:26<00:01, 66.43it/s]\r",
      " 89%|████████▉ | 891/1000 [00:26<00:01, 68.63it/s]\r",
      " 90%|████████▉ | 899/1000 [00:27<00:01, 71.23it/s]\r",
      " 91%|█████████ | 907/1000 [00:27<00:01, 73.64it/s]\r",
      " 92%|█████████▏| 915/1000 [00:27<00:01, 72.76it/s]\r",
      " 92%|█████████▏| 923/1000 [00:27<00:01, 60.40it/s]\r",
      " 93%|█████████▎| 931/1000 [00:27<00:01, 63.86it/s]\r",
      " 94%|█████████▍| 938/1000 [00:27<00:00, 65.28it/s]\r",
      " 95%|█████████▍| 946/1000 [00:27<00:00, 66.46it/s]\r",
      " 95%|█████████▌| 953/1000 [00:27<00:00, 66.13it/s]\r",
      " 96%|█████████▌| 960/1000 [00:27<00:00, 65.48it/s]\r",
      " 97%|█████████▋| 967/1000 [00:28<00:00, 64.35it/s]\r",
      " 97%|█████████▋| 974/1000 [00:28<00:00, 63.01it/s]\r",
      " 98%|█████████▊| 982/1000 [00:28<00:00, 64.71it/s]\r",
      " 99%|█████████▉| 989/1000 [00:28<00:00, 64.13it/s]\r",
      "100%|█████████▉| 996/1000 [00:28<00:00, 64.53it/s]\r",
      "100%|██████████| 1000/1000 [00:28<00:00, 35.00it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n"
     ]
    }
   ],
   "source": [
    "if ON_TRAIN:\n",
    "    # PERCENT = np.arange(0.1, 1., 0.1)\n",
    "    # PERCENT = np.linspace(0.7, 0.7, 10)\n",
    "    PERCENT = [0.1]\n",
    "    data = []\n",
    "    for p in PERCENT:\n",
    "        d = LoadData()\n",
    "        data_prior, m = d.prior_samples(p)\n",
    "        f = BayesianGMM(data_prior)\n",
    "        trace = f.training()\n",
    "        data.append([data_prior, m, trace])\n",
    "    with open('./results/data.pkl', 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "else:\n",
    "    with open('./results/data.pkl', 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    plt.figure()\n",
    "    plt.ion()\n",
    "    plt.show()\n",
    "    c = ['r', 'b', 'lime']\n",
    "    for i in range(len(data)):\n",
    "        data_prior, m, trace = data[i]\n",
    "        p = PostProcess(data_prior, trace)\n",
    "        # data_post = p.post_samples()\n",
    "        # p.joint_dist(data_post)\n",
    "        # p.marginalized_dist(data_post)\n",
    "        # p.compare_1d()\n",
    "        # prob = p.compare_2d()\n",
    "        # e = Estimation(data_prior, m)\n",
    "        # e.importance_sampling(prob)\n",
    "        # p.step()\n",
    "        j = 0\n",
    "        for var in p.var_mu:\n",
    "            plt.subplot(711 + j)\n",
    "            col = len(var)\n",
    "            [plt.plot(i, var[k], color=c[k], marker='.', markersize=10) for k in range(col)]\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_prior, m, trace = data[0]\n",
    "p = PostProcess(data_prior, trace)\n",
    "p.compare_1d()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
